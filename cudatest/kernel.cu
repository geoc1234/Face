/*

Boost Software License - Version 1.0 - August 17th, 2003

Permission is hereby granted, free of charge, to any person or organization
obtaining a copy of the software and accompanying documentation covered by
this license (the "Software") to use, reproduce, display, distribute,
execute, and transmit the Software, and to prepare derivative works of the
Software, and to permit third-parties to whom the Software is furnished to
do so, all subject to the following:

The copyright notices in the Software and this entire statement, including
the above license grant, this restriction and the following disclaimer,
must be included in all copies of the Software, in whole or in part, and
all derivative works of the Software, unless such copies or derivative
works are solely in the form of machine-executable object code generated by
a source language processor.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
FITNESS FOR A PARTICULAR PURPOSE, TITLE AND NON-INFRINGEMENT. IN NO EVENT
SHALL THE COPYRIGHT HOLDERS OR ANYONE DISTRIBUTING THE SOFTWARE BE LIABLE
FOR ANY DAMAGES OR OTHER LIABILITY, WHETHER IN CONTRACT, TORT OR OTHERWISE,
ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER
DEALINGS IN THE SOFTWARE.

*/

#include "cuda_runtime.h"
#include "device_launch_parameters.h"

#include <stdio.h>
#include <iostream>
#include <opencv2/opencv.hpp>
#include <opencv2/face.hpp>
#include <opencv2/highgui.hpp>
#include <dlib/opencv.h>
#include <dlib/image_io.h>
#include <dlib/image_processing.h>
#include <dlib/dnn.h>
#include <dlib/gui_widgets.h>
#include <dlib/dnn.h>
#include <dlib\clustering\chinese_whispers.h>

struct Faces_t
{
	std::string label;
	dlib::matrix<dlib::rgb_pixel, 150, 150> image;
	dlib::matrix<float, 128, 1> descriptors;
};
struct Clips_t
{
	float x1, x2, y1, y2;
};

template <template <int, template<typename>class, int, typename> class block, int N, template<typename>class BN, typename SUBNET>
using residual = dlib::add_prev1<block<N, BN, 1, dlib::tag1<SUBNET>>>;

template <template <int, template<typename>class, int, typename> class block, int N, template<typename>class BN, typename SUBNET>
using residual_down = dlib::add_prev2<dlib::avg_pool<2, 2, 2, 2, dlib::skip1<dlib::tag2<block<N, BN, 2, dlib::tag1<SUBNET>>>>>>;

template <int N, template <typename> class BN, int stride, typename SUBNET>
using block = BN<dlib::con<N, 3, 3, 1, 1, dlib::relu<BN<dlib::con<N, 3, 3, stride, stride, SUBNET>>>>>;

template <int N, typename SUBNET> using ares = dlib::relu<residual<block, N, dlib::affine, SUBNET>>;
template <int N, typename SUBNET> using ares_down = dlib::relu<residual_down<block, N, dlib::affine, SUBNET>>;

template <typename SUBNET> using alevel0 = ares_down<256, SUBNET>;
template <typename SUBNET> using alevel1 = ares<256, ares<256, ares_down<256, SUBNET>>>;
template <typename SUBNET> using alevel2 = ares<128, ares<128, ares_down<128, SUBNET>>>;
template <typename SUBNET> using alevel3 = ares<64, ares<64, ares<64, ares_down<64, SUBNET>>>>;
template <typename SUBNET> using alevel4 = ares<32, ares<32, ares<32, SUBNET>>>;

using anet_type = dlib::loss_metric<dlib::fc_no_bias<128, dlib::avg_pool_everything<
	alevel0<
	alevel1<
	alevel2<
	alevel3<
	alevel4<
	dlib::max_pool<3, 3, 2, 2, dlib::relu<dlib::affine<dlib::con<32, 7, 7, 2, 2,
	dlib::input_rgb_image_sized<150>
	>>>>>>>>>>>>;


int main(int argc, char** argv)
{
	//included to check that dlib will use cuda
	std::cout << dlib::cuda::get_num_devices();
	dlib::cuda::set_device(0);
	std::cout << dlib::cuda::get_device_name(0);

	//the neural net parameters for the trained network to detect faces in an image 
	std::string model = "c:/users/geo/datasets/deploy.prototxt.txt";
	std::string dataset = "c:/users/geo/datasets/res10_300x300_ssd_iter_140000.caffemodel";
	cv::dnn::Net Net_SSD = cv::dnn::readNetFromCaffe(model, dataset);

	std::vector<Clips_t> vector_of_clips; //the face regions within the collage
	std::vector<Faces_t> vector_of_Faces; //the individual faces for storage
	std::vector<Faces_t> vector_of_camFaces; //the individual faces from the cam
	std::vector<Clips_t> vector_of_camclips; //the face regions within the collage

/* -------------Not Used; using dlib instead.  Here for future reference -----------
	//the container for the landmarks returned by OpenCV facemark algorithm
	cv::Ptr<cv::face::Facemark> facemark = cv::face::FacemarkLBF::create();
	facemark->loadModel("c:/users/geo/datasets/gsoc2017/data/lbfmodel.yaml");
	std::vector<std::vector<cv::Point2f>> landmarks;
*/

//step 1: open the collage & show it for verification it was read properly.  Provide for a default image for convenience
	if (argc < 2) argv[1] = "c:/users/geo/datasets/bald_guys2.jpg";
	cv::Mat	image = cv::imread(argv[1]);
	cv::imshow("Faces", image);
	cv::waitKey(10);

	//step 2: find the faces & draw rectangles around them
		//the caffe model inputs image data in a blob structure.
	cv::Mat blob = cv::dnn::blobFromImage(image, 1.0, cv::Size(300, 300), cv::Scalar(127.5, 127.5, 127.5), false, false);

	//input the blob into the caffe model for face detection
	Net_SSD.setInput(blob);

	//propogate the blob forward through the neural net to get predictions of where faces exist in the image
	cv::Mat	Net_SSD_predict = Net_SSD.forward();

	//the results are accessed via a matrix structure.	TODO: add a reference to the documentation on the structure of this matrix
	cv::Mat detectionMat(Net_SSD_predict.size[2], Net_SSD_predict.size[3], CV_32F, Net_SSD_predict.ptr<float>());

	//post process the results by drawing a box around each face in the image
	float frameWidth = image.cols;
	float frameHeight = image.rows;

	for (int i = 0; i < detectionMat.rows; i++)
	{
		if (detectionMat.at<float>(i, 2) >= 0.5)
		{
			Clips_t res;
			res.x1 = static_cast<int>(detectionMat.at<float>(i, 3) * frameWidth);
			res.y1 = static_cast<int>(detectionMat.at<float>(i, 4) * frameHeight);
			res.x2 = static_cast<int>(detectionMat.at<float>(i, 5) * frameWidth);
			res.y2 = static_cast<int>(detectionMat.at<float>(i, 6) * frameHeight);
			vector_of_clips.push_back(res);

			cv::rectangle(image, cv::Rect(res.x1, res.y1, res.x2 - res.x1, res.y2 - res.y1), cv::Scalar(255, 0, 0), 1, 8, 0);
		}
	}

	// show the images with all faces enclosed by their associated rectangle
//		cv::imshow("Faces", image);
//		cv::waitKey(10);

	//step 3: Align the faces
		// first convert to dlib image format
	dlib::matrix<dlib::rgb_pixel> dimage;
	dlib::cv_image<dlib::rgb_pixel> temp(image);
	dlib::assign_image(dimage, temp);

	//show the recast image
//		dlib::image_window win;
//		win.set_image(dimage);
//		win.show();

	Faces_t dface;

	//use the 5 point landmark model to align the faces forward
	dlib::shape_predictor sp;
	dlib::deserialize("c:/users/geo/datasets/shape_predictor_5_face_landmarks.dat") >> sp;
	std::vector<dlib::matrix<dlib::rgb_pixel, 150, 150>> dlib_faces;

	for (int i = 0; i < vector_of_clips.size(); i++)
	{
		auto shape = sp(dimage, dlib::rectangle(vector_of_clips.at(i).x1,
			vector_of_clips.at(i).y1,
			vector_of_clips.at(i).x2,
			vector_of_clips.at(i).y2));
		dlib::extract_image_chip(dimage, get_face_chip_details(shape, 150, 0.25), dface.image);
		std::stringstream strs;
		strs << "face_" << i;
		dface.label == strs.str();

		vector_of_Faces.push_back(dface);
		dlib_faces.push_back(dface.image); // will be useful in next step
	}
	//step 4: get the landmarks
	anet_type net;
	dlib::deserialize("c:/users/geo/datasets/dlib_face_recognition_resnet_model_v1.dat") >> net;
	std::vector<dlib::matrix<float, 0, 1>> face_descriptors;
	face_descriptors = net(dlib_faces);
	for (int i = 0; i < face_descriptors.size(); i++)
	{
		vector_of_Faces[i].descriptors = face_descriptors[i];
	}

	//step 5: test the landmarks
	std::vector<dlib::sample_pair> edges;

	for (size_t i = 0; i < face_descriptors.size(); ++i)
	{
		for (size_t j = i; j < face_descriptors.size(); ++j)
		{
			// Faces are connected in the graph if they are close enough.  Here we check if
			// the distance between two face descriptors is less than 0.6, which is the
			// decision threshold the network was trained to use.  Although you can
			// certainly use any other threshold you find useful.
			if (dlib::length(face_descriptors[i] - face_descriptors[j]) < 0.6)
			{
				edges.push_back(dlib::sample_pair(i, j));
				// std::cout << i << " possible match with " << j << "\n" << std::endl;
			}
		}
	}

	std::vector<unsigned long> labels;
	const auto num_clusters = dlib::chinese_whispers(edges, labels);
	// This will correctly indicate that there are 4 people in the image.
	std::cout << "number of people found in the image: " << num_clusters << std::endl;

	// Now let's display the face clustering results on the screen.  You will see that it
	// correctly grouped all the faces.
	std::vector<dlib::image_window> win_clusters(num_clusters);
	for (size_t cluster_id = 0; cluster_id < num_clusters; ++cluster_id)
	{
		std::vector<dlib::matrix<dlib::rgb_pixel>> temp;
		for (size_t j = 0; j < labels.size(); ++j)
		{
			if (cluster_id == labels[j])
				temp.push_back(dlib_faces[j]);
		}
		win_clusters[cluster_id].set_title("face cluster " + dlib::cast_to_string(cluster_id));
		win_clusters[cluster_id].set_image(tile_images(temp));
		std::string identity;
		std::cout << "Enter the name of this person (blank for unknown) " << std::endl;
		std::cin >> identity;
		std::cin.get();

		for (size_t j = 0; j < labels.size(); ++j)
		{
			if (cluster_id == labels[j]) vector_of_Faces[j].label = identity;
		}
	}

	std::ofstream wf("c:/users/geo/datasets/faces.dat", std::ios::out | std::ios::binary);
	if (!wf)
	{
		std::cout << "Cannot open file!" << std::endl;
		return 1;
	}

	std::int16_t size_vof = vector_of_Faces.size();
	std::int16_t num, label_size, descriptor_size, image_size;
	num = vector_of_Faces.size();
	std::cout << num;

	wf.write((char*)&num, sizeof(num));

	for (int i = 0; i < num; i++)
	{
		wf.write((char*)&vector_of_Faces[i], sizeof(Faces_t));
	}

	wf.close();
	if (!wf.good()) {
		std::cout << "Error occurred at writing time!" << std::endl;
		return 1;
	}

	// vector_of_Faces.clear();

	std::ifstream rf("c:/users/geo/datasets/faces.dat", std::ios::in | std::ios::binary);
	if (!rf)
	{
		std::cout << "Cannot open file!" << std::endl;
		return 1;
	}

	rf.read((char*)&num, sizeof(num));

	for (int i = 0; i < num; i++)
	{
		rf.read((char*)&vector_of_Faces[i], sizeof(Faces_t));
	}
	rf.close();
	if (!rf.good()) {
		std::cout << "Error occurred at reading time!" << std::endl;
		return 1;
	}

	std::vector<dlib::image_window> guest_win(num);
	for (int i = 0; i < num; i++)
	{
		guest_win[i].set_image(vector_of_Faces[i].image);
		guest_win[i].set_title(vector_of_Faces[i].label);
		guest_win[i].show();
		guest_win[i].DO_NOT_CLOSE_WINDOW;
	}

	//step 6: start the cam
	//webcam can generate data stream
	cv::VideoCapture webcam(0);
	cv::Mat cam_image;

	while (true)
	{
		vector_of_clips.clear();
		webcam >> cam_image;
		//cam_image = cv::imread("c:/users/geo/datasets/bald_guys2.jpg");
		int frameHeight = cam_image.rows;
		int frameWidth = cam_image.cols;
		if (frameWidth > 0 && frameHeight > 0)
		{
			cv::imshow("Faces", cam_image);
			cv::waitKey(5);

			//the caffe model inputs image data in a blob structure.
			blob = cv::dnn::blobFromImage(cam_image, 1.0, cv::Size(300, 300), cv::Scalar(127.5, 127.5, 127.5), false, false);

			//input the blob into the caffe model for face detection
			Net_SSD.setInput(blob);

			//propogate the blob forward through the neural net to get predictions of where faces exist in the image
			cv::Mat	Net_SSD_predict = Net_SSD.forward();

			//the results are accessed via a matrix structure.	TODO: add a reference to the documentation on the structure of this matrix
			cv::Mat detectionMat(Net_SSD_predict.size[2], Net_SSD_predict.size[3], CV_32F, Net_SSD_predict.ptr<float>());

			//post process the results by drawing a box around each face in the immage
			for (int i = 0; i < detectionMat.rows; i++)
			{
				if (detectionMat.at<float>(i, 2) >= 0.5)
				{
					Clips_t res;

					res.x1 = static_cast<int>(detectionMat.at<float>(i, 3) * frameWidth);
					res.y1 = static_cast<int>(detectionMat.at<float>(i, 4) * frameHeight);
					res.x2 = static_cast<int>(detectionMat.at<float>(i, 5) * frameWidth);
					res.y2 = static_cast<int>(detectionMat.at<float>(i, 6) * frameHeight);

					vector_of_camclips.push_back(res);
					cv::rectangle(cam_image, cv::Rect(res.x1, res.y1, res.x2 - res.x1, res.y2 - res.y1), cv::Scalar(255, 0, 0), 1, 8, 0);

				}
			}

			cv::imshow("Faces", cam_image);
			cv::waitKey(5);

			dlib::matrix<dlib::rgb_pixel> dcam_image;
			dlib::cv_image<dlib::rgb_pixel> cam_temp(cam_image);
			dlib::assign_image(dcam_image, cam_temp);

			Faces_t dcam_face;

			//use the 5 point landmark model to align the faces forward
			dlib::shape_predictor cam_sp;
			dlib::deserialize("c:/users/geo/datasets/shape_predictor_5_face_landmarks.dat") >> cam_sp;
			std::vector<dlib::matrix<dlib::rgb_pixel, 150, 150>> dlib_camfaces;

			for (int i = 0; i < vector_of_camclips.size(); i++)
			{
				auto shape = cam_sp(dcam_image, dlib::rectangle(vector_of_camclips.at(i).x1,
					vector_of_camclips.at(i).y1,
					vector_of_camclips.at(i).x2,
					vector_of_camclips.at(i).y2));
				dlib::extract_image_chip(dcam_image, get_face_chip_details(shape, 150, 0.25), dcam_face.image);
				std::stringstream strs;
				strs << "face_" << i;
				dface.label == strs.str();

				vector_of_camFaces.push_back(dcam_face);
				dlib_camfaces.push_back(dcam_face.image); // will be useful in next step
			}
			anet_type cam_net;
			dlib::deserialize("c:/users/geo/datasets/dlib_face_recognition_resnet_model_v1.dat") >> cam_net;
			std::vector<dlib::matrix<float, 0, 1>> cam_face_descriptors;
			cam_face_descriptors = cam_net(dlib_camfaces);
			for (int i = 0; i < cam_face_descriptors.size(); i++)
			{
				vector_of_camFaces[i].descriptors = cam_face_descriptors[i];
			}

			//step 5: test the landmarks
			std::vector<dlib::sample_pair> cam_edges;

			for (size_t i = 0; i < cam_face_descriptors.size(); ++i)
			{
				for (size_t j = 0; j < face_descriptors.size(); ++j)
				{
					// Faces are connected in the graph if they are close enough.  Here we check if
					// the distance between two face descriptors is less than 0.6, which is the
					// decision threshold the network was trained to use.  Although you can
					// certainly use any other threshold you find useful.
					if (dlib::length(cam_face_descriptors[i] - face_descriptors[j]) < 0.6)
					{
						cam_edges.push_back(dlib::sample_pair(i, j));
						std::cout << i << " possible match with " << vector_of_Faces[j].label << "\n" << std::endl;
					}
				}
			}

		}
	}

	std::cout << "hit enter to terminate" << std::endl;
	std::cin.get();
	std::cin.get();
	return 0;



}